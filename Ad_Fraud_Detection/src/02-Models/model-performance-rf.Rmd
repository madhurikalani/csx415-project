---
title: "model-performance-random-forest"
author: "Madhuri Kalani"
date: "April 21, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Loading the Project

The first step of model training is to load the project. It includes installing all the necessary libraries and data munging to prepare the training and test data sets, doing exploratory data analysis to study the data and feature selection.

```{r p0, echo =FALSE, warning=FALSE, include=FALSE}
setwd("C:/Users/akash/Documents/Madhuri stuff/Data_Science_Intro_R/Ad_Fraud_Detection")

library('ProjectTemplate')
load.project()

library('caret')



```

## Model Training using Random forest

The caret package in R provides a powerful framework for growing classification trees.

```{r c1, warning = FALSE}

#Tree Model to Classify - Random forest


train_control <- trainControl(method="cv", number=3)

# train the model

model_rf <- train(as.factor(is_attributed)~app + channel + ip +device + os + click_time, data=df.train, method="rf", trControl=train_control)

```

### Summary of Trained Random Forest Model
```{r p10,echo =FALSE , warning=FALSE}

# Results of Model Training
model_rf
```


### Plotting the Trained Model

```{r p11,echo =FALSE , warning=FALSE}
plot(model_rf)

```

### Fitting the Trained RF Model
```{r p12,echo =FALSE , warning=FALSE}
predict_rf <- predict(model_rf, df.test)
```
 ## Evaluation 
### Step 1 Building a Confusion Matrix


It is a tabular representation of Actual vs Predicted values. This helps to find the accuracy of the model and avoid overfitting. 
```{r p13,echo =FALSE , warning=FALSE}
print("Confusion Matrix for rf")
cm_rf <- confusionMatrix(table(as.factor(df.test$is_attributed),predict_rf))
cm_rf
```
### Step 2 Finding Cohen's Kappa

```{r p14,echo =FALSE , warning=FALSE}
print("kappa value for Random Forest")
cm_rf$overall[2]
```
### Step 3 Finding Accuracy Value
```{r p15,echo =FALSE , warning=FALSE}
print("Accuracy value for Random Forest")
cm_rf$overall[1]
```
### Step 4 Building a ROC Curve

An ROC curve demonstrates several things for 2-class classification algorithms:

It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).
The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.
The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.

``` {r p16,echo =FALSE,  warning = FALSE, message = FALSE}

library(pROC)
plot(roc(df.test$is_attributed, as.numeric(predict_rf), direction="<"),
      col="yellow", lwd=3, main="ROC Curve for Random Forest")


```


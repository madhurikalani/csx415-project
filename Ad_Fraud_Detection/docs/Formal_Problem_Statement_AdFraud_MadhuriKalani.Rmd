---
title: "Formal_Problem_Statement_AdFraud"
author: "Madhuri Kalani"
date: "April 10, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Table of Contents

1.	Business Problem Statement

2.	Project Objectives

3.  Stakeholders

4.	Business Value - Benefits 

5.	Success Criteria and Metrics	

6.	Project Risks and Mitigation Strategies

7.  Risks to Model Deployment

8.  Schedule / Milestones- Timeline for Delivery 

9.  Project Communication Plan


## Business Problem Statement

Advertisement click fraud is very common among companies who want to direct traffic to their websites or mobile apps. Most companies pay advertisers on per click basis which can be a costly affair in a situation where it is difficult to distinguish between legitimate clicks from fraudalent ones. Companies have deviced an approach to block IP addresses which produce lot of clicks but that was not sufficient to solve this problem.
However, if we build a model we can predict whether a user will actually download an app after clicking any adverstisement or not.

In this project, TData (big data services provider) covers 70% of active mobile devices and handles 3 billion ad-clicks per day, of which over 90% are fraudulent. The team would now like to build a model which will help them to predict whether the user will download the app or not. 

## Project Objectives

1.	Build a model to predict whether the user will download the mobile app after clicking the advertisement. The company has provided a generous dataset covering 200 million clicks in 4 days. The dataset is quite huge i.e. ~8 GB and 200 million records (train + test) and fits the definition of rare events data where 99.8% records are negative and only 0.2% records are positive. The dataset is interesting and unique because of rare events data/ highly unbalanced classes.
 
2. The main tasks would be data processing, feature selection, data analysis, model training and selection, model evaluation and predictions.	


## Stakeholders


```{r, echo=FALSE, results='asis'}

text_tbl <- data.frame(
  Stakeholder = c("Dean Mazel", "Jenny Parker", "Hiabc Jones","Madhuri Kalani","Nick Thompson and  Richard Wright"),
  Department = c( "Vice President /Sponsor (End User)",
    "Marketing Project Manager ",
    "Analytics Manager ", 
    "Data Scientist"
 ,"SME and Lead" ),
  Role = c(
    "He is the marketing head at TData and has given approval for this project",
    "She is the Project Manager and will work closely with Analytics manager of data science team.She will track the timelines of the ongoing project, and ensure that the proposed project aligns with objectives.", "He will be managing resources and funding of the project. His role will also include reporting to the project manager and marketing head", "She will analyze the data, build the model, will evaluate the performance of the model and create necessary documentation for future use"," Being subject matter experts, they will identifying deployment end points, business risks. With the help of feature specification and model spec. documents, they will lead the deployment of the model."
  )
)
knitr::kable(text_tbl, longtable = TRUE)

 # knitr::kable_styling(full_width = F) %>%
#  column_spec(1, bold = T, border_right = T) %>%
 # column_spec(2, width = "30em", background = "yellow")

```



## Business Value - Benefits 

1.	Dean Mazel and his team will gain visibility on ad-traffic fraud.

2.	Marketing team can track ad-frauds and block ip,device, users corresponding to negative event with a opportunity of reducing ad-fraud by a minimum of 60%.

3. Company's annual spend of $10M on handling user clicks will not go to waste.

4. Team will have a better understanding on attributes contributing to ad-fraud and will able to prevent the risks involved with it. However, the data science team will have to keeping evaluating the performance of the model from time to time to avoid any chance of fraudulent in the ad world.


## Success Criteria and Metrics 

The evaluation of the the model affects the time and money spend on handling user clicks and negative predictions add ons to the cost. If the model identifies a false download even when the user actually downloaded the app, it will add on to the number of fradulent users. But if it predicts positive values even when user did not download the app, the cost is increased to handle such event.

1.	Team is able to channelize the annual $10M spend on handling user clicks with ad-fraudulent to be reduced by 60%. If these outcomes are not met, company will loose it's mobile users and partners who invest millions in advertising their mobile ads. We should also take care of the cost involved in building a model and maintaining it. proper methods should be used to evaluate the actual savings.

2.	Data Science team is able to train the model that performs the best and predicts the two classes which depicts user clicks. They  measure accuracy and kappa values to select a model best suited for this classification model. Provide evaluation results which include confusion matrix, ROC curve needs to be consistent over time. 


## Project Risks and Mitigation Strategies

1.	If the success metrics is not met, the annual spend of $10M digital ad spending is at risk. The company will continue loosing money to ad-fradulent until a solution/ model strong enough to identify actual user clicks is being implemented.
2.	Additional cost of resources, time and technology used thus far to formulate a solution.
3.	In the model performance is poor, the team will continue working on ways to improve the performance ie. getting more data, adding new features or training the data with a different model.


## Risks to Model Deployment

The risks involved with model development could be:

1.	Difficulty to scale up the model with large datasets
2.	Issue with calibrating the model
3.	Errors while evaluating the model
4.	Picking up a wrong model for this project implementation


## Schedule / Milestones- Timeline for delivery of the project

The project involves 6 phases which are:

1.	Inception - March 27th to April 1st, 2018

2.	Formalization - April 2nd to April 10th, 2018 

3.	Model Development - April 10th to May 12th, 2018 

4.	Evaluating Results - May 13th to May 20th, 2018

5.	Model Deployment -  May 21st to May 22nd, 2018

6.	Model Management - May 23rd to May 28th, 2018

7.	Building Reports - May 29th to May 30th, 2018 

## Project Communication Plan


```{r three, echo=FALSE, results='asis'}


text_tbl2 <- data.frame(
  Focus_Area = c("Stakeholder Updates", "Project Manger Updates", "Team Progress/Reports","Communication Plan","Deployment Team Update"),
  Communication_Plan = c( "Bi-weekly updates given by Jenny via tele-conference meeting","Updates meeting every week (Tuesday)","Meeting every evening to discuss the updates and any hurdles associated with the project conducted by Analytics Manger for the model development team ", "Regular meeting on these above mentioned days. Meeting will be conducted in meeting rooms, please book in advance and if none available it will be conducted near manager's desk. Development team can also discuss problems over Slack project group or email","Deployment team to be notified a month in advance about upcoming deployment or atleast two weeks before the deployment")
)
knitr::kable(text_tbl2, longtable = TRUE)

 # knitr::kable_styling(full_width = F) %>%
#  column_spec(1, bold = T, border_right = T) %>%
 # column_spec(2, width = "30em", background = "yellow")

```








